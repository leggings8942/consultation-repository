{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "from dotenv import load_dotenv\n",
    "import random, string\n",
    "\n",
    "import pyspark as ps\n",
    "from pyspark.sql import SparkSession, types\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: mapreduce.fileoutputcommitter.marksuccessfuljobs\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/09 18:40:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "ps_conf = ps.SparkConf()\\\n",
    "            .set(\"spark.sql.sources.commitProtocolClass\", \"org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\")\\\n",
    "            .set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")\\\n",
    "            .set(\"spark.sql.shuffle.partitions\", 100)\\\n",
    "            .set(\"spark.sql.dynamicPartitionPruning.enabled\", True)\n",
    "            # '_started'と'_committed_'で始まるファイルを書き込まないように設定\n",
    "            # '_SUCCESS'で始まるファイルを書き込まないように設定\n",
    "            # パーティション数を調整する\n",
    "            # 動的パーティションプルーニングの有効化\n",
    "spark = SparkSession.builder.config(conf=ps_conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(join(os.getcwd(), '.env'))\n",
    "BASE_PATH = os.environ.get(\"BASE_PATH\")\n",
    "WORK_PATH = BASE_PATH + os.environ.get(\"WORK_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回遊1階層のテストデータ生成\n",
    "\n",
    "# 75端末、24時間分\n",
    "unit_id_num = 75*24\n",
    "\n",
    "random_unit_list = [''.join(random.choices(string.digits, k=5)) for _ in range(0, unit_id_num)]\n",
    "tmp_list = [[unit_id, random.random()] for unit_id in random_unit_list]\n",
    "\n",
    "df_schema = types.StructType([\n",
    "        types.StructField('ORIGIN',      types.StringType(), False),\n",
    "        types.StructField('移動影響量',    types.FloatType(),  False),\n",
    "    ])\n",
    "df_migrate1 = spark.createDataFrame(tmp_list, df_schema)\n",
    "df_migrate1\\\n",
    "\t.orderBy(col('ORIGIN').asc())\\\n",
    "    .toPandas()\\\n",
    "    .to_csv(WORK_PATH + 'csv_data/test_1.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 18:40:56 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:695)\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:660)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:636)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:582)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:541)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:65)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 回遊2階層のテストデータ生成\n",
    "\n",
    "df_schema = types.StructType([\n",
    "        types.StructField('ORIGIN',      types.StringType(), False),\n",
    "        types.StructField('DESTINATION', types.StringType(), False),\n",
    "        types.StructField('移動影響量',    types.FloatType(),  False),\n",
    "    ])\n",
    "df_migrate2 = spark.createDataFrame([], df_schema)\n",
    "\n",
    "for unit_id, move in tmp_list:\n",
    "    df_tmp      = df_migrate1\\\n",
    "        \t\t\t\t.withColumn('DESTINATION', F.lit(unit_id))\\\n",
    "\t\t\t\t\t\t.withColumn('移動影響量',    col('移動影響量') * move)\n",
    "    df_migrate2 = df_migrate2.unionByName(df_tmp)\n",
    "\n",
    "df_migrate2\\\n",
    "    .orderBy(col('ORIGIN').asc(), col('DESTINATION').asc())\\\n",
    "    .toPandas()\\\n",
    "    .to_csv(WORK_PATH + 'csv_data/test_2.csv', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
