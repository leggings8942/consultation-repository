{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pyspark as ps\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import SparkSession, DataFrame, types\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_conf = ps.SparkConf()\\\n",
    "            .set(\"spark.sql.sources.commitProtocolClass\", \"org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\")\\\n",
    "            .set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")\\\n",
    "            .set(\"spark.sql.shuffle.partitions\", 100)\\\n",
    "            .set(\"spark.sql.dynamicPartitionPruning.enabled\", True)\n",
    "            # '_started'と'_committed_'で始まるファイルを書き込まないように設定\n",
    "            # '_SUCCESS'で始まるファイルを書き込まないように設定\n",
    "            # パーティション数を調整する\n",
    "            # 動的パーティションプルーニングの有効化\n",
    "spark = SparkSession.builder.config(conf=ps_conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(join(os.getcwd(), '.env'))\n",
    "BASE_PATH     = os.environ.get(\"BASE_PATH\")\n",
    "WORK_PATH     = BASE_PATH + os.environ.get(\"WORK_PATH\")\n",
    "PROJECT_NAME  = os.environ.get(\"PROJECT_NAME\")\n",
    "INSTRUCT_PATH = WORK_PATH + PROJECT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_migration(df_devided:DataFrame) -> DataFrame:\n",
    "    # df_deviedに期待する構成\n",
    "    #  | 'ORIGIN' | 'DESTINATION' | '移動影響量'\n",
    "    # 0, 30943,     30943,          0.0\n",
    "    # 1, 30943,     30984,          0.1013\n",
    "    # 2, 30943,     30985,          0.1043\n",
    "    # 3, 30943,     31177,          0.0\n",
    "    # ・\n",
    "    # ・\n",
    "    # ・\n",
    "    \n",
    "    # 出力用データフレームの構成\n",
    "    spark      = SparkSession.getActiveSession()\n",
    "    df_schema  = types.StructType([\n",
    "        types.StructField('ORIGIN',         types.StringType(),    False),\n",
    "        types.StructField('VIA_1',          types.StringType(),    False),\n",
    "        types.StructField('DESTINATION',    types.StringType(),    False),\n",
    "        types.StructField('移動影響量_VIA_1', types.FloatType(),     False),\n",
    "    ])\n",
    "    \n",
    "    # 経由地1のunitリスト\n",
    "    v_unit_list  = sorted(df_devided.select('DESTINATION').drop_duplicates().rdd.flatMap(lambda x: x).collect())\n",
    "    \n",
    "    # 空のデータフレームの作成\n",
    "    df_migrate = spark.createDataFrame([], df_schema)\n",
    "    for v_unit in v_unit_list:\n",
    "        # 1) 2階層回遊におけるDESTINATIONのunitを抽出\n",
    "        df_v1_to_d = df_devided.filter(col('DESTINATION') == v_unit)\\\n",
    "                                .withColumnRenamed('移動影響量', '移動影響量_VIA_0')\\\n",
    "                                .select(['ORIGIN', '移動影響量_VIA_0'])\n",
    "        \n",
    "        # 2) 1)のunitを出発地とするirfを抽出\n",
    "        df_v1_to_o = df_devided.filter(col('ORIGIN')      == v_unit)\\\n",
    "                                .withColumnsRenamed({'ORIGIN':'VIA_1', '移動影響量':'移動影響量_VIA_1'})\\\n",
    "                                .select(['VIA_1', 'DESTINATION', '移動影響量_VIA_1'])\n",
    "        \n",
    "        # 3) 交差結合する\n",
    "        df_v1 = df_v1_to_d.crossJoin(df_v1_to_o)\n",
    "        \n",
    "        # 4) 出発地・経由地1のirf と 経由地1・目的地のirf を掛ける\n",
    "        df_v1 = df_v1.withColumn('移動影響量_VIA_1', col('移動影響量_VIA_0') * col('移動影響量_VIA_1'))\n",
    "        \n",
    "        # 5) 列を整える\n",
    "        df_v1 = df_v1.select(['ORIGIN', 'VIA_1', 'DESTINATION', '移動影響量_VIA_1'])\n",
    "        \n",
    "        # 6) 縦結合する\n",
    "        df_migrate = df_migrate.union(df_v1)\n",
    "\n",
    "    # データの整形を行う\n",
    "    df_migrate = df_migrate\\\n",
    "                    .orderBy(col('ORIGIN').asc(), col('VIA_1').asc(), col('DESTINATION').asc())\n",
    "    \n",
    "    # df_migrateに期待する構成\n",
    "    #  | 'ORIGIN' | 'VIA_1' | 'DESTINATION' | '移動影響量_VIA_1'\n",
    "    # 0, 30943,     30984,    30943,          0.0\n",
    "    # 1, 30943,     30984,    30984,          0.01087849\n",
    "    # 2, 30943,     30984,    30985,          0.01087849\n",
    "    # 3, 30943,     30984,    31177,          0.0\n",
    "    # ・\n",
    "    # ・\n",
    "    # ・\n",
    "    \n",
    "    return df_migrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_terminal = 50\n",
    "df_schema = types.StructType([\n",
    "        types.StructField('ORIGIN',      types.StringType(), False),\n",
    "        types.StructField('DESTINATION', types.StringType(), False),\n",
    "        types.StructField('移動影響量',    types.FloatType(),  False),\n",
    "    ])\n",
    "df_test_data = spark.read\\\n",
    "                .option('inferSchema', 'True')\\\n",
    "                .option('header', 'True')\\\n",
    "                .csv(INSTRUCT_PATH + f'csv_data/test_terminal{unit_terminal}.csv', schema=df_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_migrate = calc_migration(df_test_data)\n",
    "df_migrate.persist(StorageLevel.MEMORY_ONLY)\n",
    "df_migrate.count()\n",
    "\n",
    "df_migrate\\\n",
    "    .toPandas()\\\n",
    "    .to_csv(INSTRUCT_PATH + 'csv_data/test_migrate.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
